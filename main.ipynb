import pymupdf as fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
import chromadb
from llama_cpp import Llama

# PDF â†’ Text Extraction (Auto)
pdf_path = "/content/erp_auro.pdf"  
doc = fitz.open(pdf_path)
text = ""
for page in doc:
    text += page.get_text()

print(" PDF Text Extracted Successfully")

#Chunk Text & Create Embeddings
chunks = [text[i:i+512] for i in range(0, len(text), 512)]
embedder = SentenceTransformer("all-MiniLM-L6-v2")  # Very fast & accurate
chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_or_create_collection("pdf_chunks")

#  Store Embeddings (Skip if Already Done)
if len(collection.get(include=["metadatas", "documents"])['ids']) == 0:
    for idx, chunk in enumerate(chunks):
        embedding = embedder.encode(chunk).tolist()
        collection.add(
            ids=[f"chunk_{idx}"],
            documents=[chunk],
            embeddings=[embedding],
            metadatas=[{"source": f"chunk_{idx}"}]
        )
print(" Embeddings Stored Successfully")

# Load Your Local Mistral GGUF Model
model_path = "/content/drive/MyDrive/mistral-model/mistral.gguf" 
llm = Llama(
    model_path=model_path,
    n_ctx=4096,
    n_threads=8  # Adjust based on Colab runtime
)
print("Model Loaded Successfully")

# Ask Questions & Retrieve Answers
query = input("Ask your question about the PDF: ")
query_embedding = embedder.encode(query).tolist()

results = collection.query(
    query_embeddings=[query_embedding],
    n_results=3
)
retrieved_text = "\n\n".join(results["documents"][0])

prompt = f"""
You are a document assistant.

Document Content:
{retrieved_text}

Question: {query}

Answer the question based only on the document content above.
"""

response = llm(prompt, temperature=0.0, max_tokens=512)
answer = response["choices"][0]["text"].strip()

print("\n Answer:", answer)
